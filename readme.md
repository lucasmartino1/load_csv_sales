# *Ingest√£o de CSV de Vendas*

Ol√° a todos üëã, este reposit√≥rio cont√©m um projeto de ingest√£o de um arquivo CSV de vendas, simulando uma tarefa comum em diversos ambientes empresariais. O objetivo √© possibilitar uma ingest√£o orquestrada e controlada, utilizando componentes da Google Cloud Platform (GCP). Voc√™ pode adaptar o projeto para realiz√°-lo em outros provedores de nuvem como a AWS ou AZURE, ou ent√£o realizar a orquestra√ß√£o dentro do pr√≥prio ambiente da Google Cloud Platform, substituindo o Airflow pelo Cloud Composer, por exemplo. Como se trata de uma ingest√£o simples para um projeto pessoal, optei por escolher ferramentas com baixo custo. Fique √† vontade para adapt√°-las como achar melhor!

---

## *Arquitetura do Projeto*

![lucasmartino1 Sales CSV](./datapipeline.gif)

---

### **Configura√ß√£o do Ambiente**

---

1. **Instala√ß√£o**

Antes de come√ßar, certifique-se de que voc√™ tem os seguintes itens instalados:

- **Docker**: [Instala√ß√£o do Docker](https://docs.docker.com/get-docker/)
- **Docker Compose**: [Instala√ß√£o do Docker Compose](https://docs.docker.com/compose/install/)
- **Conta no Google Cloud Platform (GCP)**: Crie uma conta [aqui](https://cloud.google.com/free).

---

2. **Configure os recursos na GCP:**
   - **Crie um bucket no Google Cloud Storage:**
     ```bash
     gcloud storage buckets create <nome-do-bucket> --location=us-central1
     ```
   - **Suba o arquivo `vendas.csv` no Bucket do Google Cloud Storage:**
     ```bash
     gcloud storage cp vendas.csv gs://<nome-do-bucket>
     ```
   - **Crie um dataset no Google BigQuery:**
     ```bash
     bq --location=US mk -d <nome-do-projeto>:<nome-do-dataset>
     ```
   - **Crie um acesso no IAM da Google Cloud:**
     ```bash
     gcloud iam service-accounts create <nome-do-acesso> \
       --description="Conta de servi√ßo para ingest√£o de dados" \
       --display-name="Ingest√£o de Dados"
     ```
   - **Atribua as seguintes fun√ß√µes ao acesso:**
     ```bash
     gcloud projects add-iam-policy-binding <nome-do-projeto> \
       --member=serviceAccount:<nome-do-acesso>@<nome-do-projeto>.iam.gserviceaccount.com \
       --role=roles/bigquery.admin

     gcloud projects add-iam-policy-binding <nome-do-projeto> \
       --member=serviceAccount:<nome-do-acesso>@<nome-do-projeto>.iam.gserviceaccount.com \
       --role=roles/bigquery.dataEditor

     gcloud projects add-iam-policy-binding <nome-do-projeto> \
       --member=serviceAccount:<nome-do-acesso>@<nome-do-projeto>.iam.gserviceaccount.com \
       --role=roles/bigquery.user

     gcloud projects add-iam-policy-binding <nome-do-projeto> \
       --member=serviceAccount:<nome-do-acesso>@<nome-do-projeto>.iam.gserviceaccount.com \
       --role=roles/storage.objectAdmin

     gcloud projects add-iam-policy-binding <nome-do-projeto> \
       --member=serviceAccount:<nome-do-acesso>@<nome-do-projeto>.iam.gserviceaccount.com \
       --role=roles/storage.objectViewer
     ```
   - **Gere o arquivo `gcp-sa.json` com as credenciais de acesso:**
     ```bash
     gcloud iam service-accounts keys create gcp-sa.json \
       --iam-account=<nome-do-acesso>@<nome-do-projeto>.iam.gserviceaccount.com
     ```

3. **Configure o ambiente local:**
   - **Crie um arquivo `.env` na pasta do seu projeto com o seu ID de usu√°rio:**
     ```bash
     AIRFLOW_UID=<Seu Id>
     ```
   - **Atualize o arquivo `docker-compose.yaml` com o caminho do arquivo `gcp-sa.json`.**
   ```yaml
        volumes:
          - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
          - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
          - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
          - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
          - <Substitua Aqui!>/gcp-sa.json:/opt/airflow/gcp/gcp-sa.json:ro
   ```

4. **Suba os servi√ßos com Docker:**
   - Navegue at√© a pasta onde est√° o arquivo `docker-compose.yaml` e execute:
     ```bash
     docker compose up -d
     ```

5. **Configure o Airflow:**
   - Acesse o Airflow pelo navegador (geralmente em `http://localhost:8080`).
   - Em **Admin > Connections**, crie uma conex√£o chamada `google_cloud_default` com os seguintes par√¢metros:
     - **Conn Type:** Google Cloud Platform
     - **Project Id:** Nome do seu projeto
     - **Key path:** `/opt/airflow/gcp/gcp-sa.json`
   - Salve a conex√£o.

6. **Execute a DAG no Airflow:**
   - No painel do Airflow, encontre a DAG chamada `sales_csv_to_bq`.
   - Ative e execute a DAG.

---

## **Considera√ß√µes Finais**

Este projeto √© uma base para ingest√£o de dados de CSV no BigQuery, utilizando o Airflow para orquestra√ß√£o e o Google Cloud Platform como provedor de nuvem. Adapte conforme necess√°rio para atender √†s suas necessidades espec√≠ficas. Se tiver d√∫vidas, sinta-se √† vontade para abrir uma issue neste reposit√≥rio.


- **dags/**: Cont√©m os DAGs do Airflow.
- **logs/**: Diret√≥rio onde os logs do Airflow s√£o armazenados.
- **plugins/**: Plugins personalizados para o Airflow.

